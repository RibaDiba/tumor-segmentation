{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, cv2, os, importlib, sys, matplotlib.pyplot as plt, numpy as np\n",
    "from PIL import Image\n",
    "os.chdir('../lib')\n",
    "\n",
    "import preprocess_images, get_bounding_box\n",
    "\n",
    "importlib.reload(preprocess_images)\n",
    "importlib.reload(get_bounding_box)\n",
    "from preprocess_images import preprocess_rgb, preprocess_grayscale, preprocess_rgbd\n",
    "from get_bounding_box import get_bounding_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocess_images file as 3 functions, each cooresponding to the file type. All 3 functions are void functions that return the variables listed below. There is also an \"og\" (original) variable wihtin the python script that is not returned that can be used to find the unaltered image.\n",
    "\n",
    "Ex:\n",
    "\n",
    "Preprocess_rgb = get all rgb image data\n",
    "\n",
    "Preprocess_rgbd = get all image data infused with depth information on the blue color channel\n",
    "\n",
    "Preprocess_grayscale = get all image data that is grayscale (based on depth map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocess_rgb() missing 4 required positional arguments: 'folder_path', 'per_train', 'per_val', and 'per_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_images, train_masks, val_images, val_masks, test_images, test_masks \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: preprocess_rgb() missing 4 required positional arguments: 'folder_path', 'per_train', 'per_val', and 'per_test'"
     ]
    }
   ],
   "source": [
    "train_images, train_masks, val_images, val_masks, test_images, test_masks = preprocess_rgb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image_array(image_array_raw, image_array_binary, num_augmentations):\n",
    "    \n",
    "    aug_raw = []\n",
    "    aug_masks = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "            for i in range(len(image_array_raw) -1):\n",
    "                image_raw = image_array_raw[i]\n",
    "                image_binary = image_array_binary[i]\n",
    "\n",
    "                flipped_image_raw = cv2.flip(image_raw, 1)\n",
    "                flipped_image_binary = cv2.flip(image_binary, 1)\n",
    "\n",
    "                angle = random.uniform(-30, 30)\n",
    "                (h, w) = flipped_image_raw.shape[:2]\n",
    "                center = (w // 2, h // 2)\n",
    "\n",
    "                M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "                augmented_image_raw = cv2.warpAffine(flipped_image_raw, M, (w, h))\n",
    "                augmented_image_binary = cv2.warpAffine(flipped_image_binary, M, (w, h))\n",
    "\n",
    "                aug_raw.append(augmented_image_raw)\n",
    "                aug_masks.append(augmented_image_binary)\n",
    "\n",
    "    image_array_raw = np.concatenate((image_array_raw, np.array(aug_raw)))\n",
    "    image_array_binary = np.concatenate((image_array_binary, np.array(aug_masks)))\n",
    "\n",
    "    return image_array_raw, image_array_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply augmentation to arrays \n",
    "\n",
    "train_images, train_masks = augment_image_array(train_images, train_masks, 100)\n",
    "val_images, val_masks = augment_image_array(val_images, val_masks, 100)\n",
    "test_images, test_masks = augment_image_array(test_images, test_masks, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2526\n",
      "304\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# print new lengths \n",
    "\n",
    "print(len(train_images))\n",
    "print(len(val_images))\n",
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "training_dataset_dict = {\n",
    "    \"image\": [Image.fromarray(img) for img in train_images],\n",
    "    \"label\": [Image.fromarray(mask) for mask in train_masks],\n",
    "}\n",
    "\n",
    "val_dataset_dict = {\n",
    "    \"image\": [Image.fromarray(img) for img in val_images],\n",
    "    \"label\": [Image.fromarray(mask) for mask in val_masks],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create the dataset using the datasets.Dataset class\n",
    "training_dataset = Dataset.from_dict(training_dataset_dict)\n",
    "val_dataset = Dataset.from_dict(val_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    image = item[\"image\"]\n",
    "    ground_truth_mask = np.array(item[\"label\"])\n",
    "\n",
    "    # get bounding box prompt\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "    # prepare image and prompt for the model\n",
    "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "    # remove batch dimension which the processor adds by default\n",
    "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "    # add ground truth segmentation\n",
    "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamProcessor\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from transformers import SamModel\n",
    "import monai\n",
    "from torch.optim import Adam\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"./medsam_pretrained/\", local_files_only=True)\n",
    "\n",
    "training_dataset = SAMDataset(dataset=training_dataset, processor=processor)\n",
    "val_dataset = SAMDataset(dataset=val_dataset, processor=processor)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=training_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=2, shuffle=False) \n",
    "\n",
    "# TODO: imoort medsam pretrained on della \n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "\n",
    "# Note: Hyperparameter tuning could improve performance here\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "# config \n",
    "num_epochs = 20\n",
    "folder_path_model = ''\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "\n",
    "        # Compute loss\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "        # Backward pass (compute gradients)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    # Logging training results\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean training loss: {mean(epoch_losses)}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_losses = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                            input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                            multimask_output=False)\n",
    "\n",
    "            # Compute loss\n",
    "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "            loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    # Logging validation results\n",
    "    print(f'Mean validation loss: {mean(val_losses)}')\n",
    "    \n",
    "# save model\n",
    "torch.save(model.state_dict(), folder_path_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detetcron2-env-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
