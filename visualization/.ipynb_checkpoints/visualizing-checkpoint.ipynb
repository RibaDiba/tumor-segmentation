{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch, importlib\n",
    "import cv2, random, os\n",
    "\n",
    "import format_images\n",
    "\n",
    "importlib.reload(format_images)\n",
    "from format_images import format_depth\n",
    "\n",
    "def get_bounding_box(image_mask):\n",
    "    \n",
    "    if np.all(image_mask == 0):\n",
    "        # If all zeros, create a random bounding box\n",
    "        H, W = image_mask.shape\n",
    "        x_min = np.random.randint(0, W)\n",
    "        x_max = np.random.randint(x_min + 1, W + 1)  # Ensure x_max > x_min\n",
    "        y_min = np.random.randint(0, H)\n",
    "        y_max = np.random.randint(y_min + 1, H + 1)  # Ensure y_max > y_min\n",
    "        \n",
    "        bbox = [x_min, y_min, x_max, y_max]\n",
    "    else: \n",
    "        if len(image_mask.shape) == 2 or image_mask.shape[2] == 1:\n",
    "            gray = image_mask\n",
    "        else:\n",
    "            gray = cv2.cvtColor(image_mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if not contours:\n",
    "            return (0, 0, 0, 0)\n",
    "        \n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        bbox = [x, y, x+w, y+h]\n",
    "    \n",
    "    return bbox\n",
    "\n",
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch\n",
    "# Load the model configuration\n",
    "model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "my_mito_model = SamModel(config=model_config)\n",
    "#Update the model by loading the weights from saved file.\n",
    "my_mito_model.load_state_dict(torch.load(\"./models/SAM1.pth\"))\n",
    "\n",
    "# set the device to cuda if available, otherwise use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "my_mito_model.to(device)\n",
    "\n",
    "train_images, train_masks, val_images, val_masks, test_images, test_masks = format_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a random training example\n",
    "idx = random.randint(0, len(train_images) -1)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "\n",
    "# load image\n",
    "test_image = train_images[idx]\n",
    "\n",
    "# get box prompt based on ground truth segmentation map\n",
    "ground_truth_mask = train_masks[idx]\n",
    "\n",
    "# test bounding box \n",
    "#bbox = [0,0,256,256]\n",
    "\n",
    "pointer_prompt = [100,150]\n",
    "\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "# prepare image + box prompt for the model\n",
    "inputs = processor(test_image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "# Move the input tensor to the GPU if it's not already there\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "my_mito_model.eval()\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = my_mito_model(**inputs, multimask_output=False)\n",
    "\n",
    "# apply sigmoid\n",
    "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "bbox = prompt  # Assuming prompt is the bounding box coordinates\n",
    "rect = Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')\n",
    "axes[0].add_patch(rect)\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(np.array(test_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[1].imshow(medsam_seg, cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Mask\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[2].imshow(medsam_seg_prob)  # Assuming the second image is grayscale\n",
    "axes[2].set_title(\"Probability Map\")\n",
    "\n",
    "# Plot the original mask\n",
    "axes[3].imshow(train_masks[idx], cmap = \"gray\")\n",
    "axes[3].set_title(\"orginal mask\")\n",
    "\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detetcron2-env-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
